# -*- coding: utf-8 -*-
# %% [markdown]
# # ðŸš€ Snorkel Ludwig Tutorial: Spam Classification

# %% [markdown]
# In this tutorial, we will demonstrate how Snorkel labels can be easily integrated with [Ludwig](https://uber.github.io/ludwig), a simple, code-free interface for training deep machine learning models.  This package allows for out-of-the-box experimentation with different neural network architectures -- even for datapoints with many modalities! -- via a simple command line interface.
#
# We'll start with our spam detection task from the [Spam tutorial](https://snorkel.org/use-cases/spam-tutorial).  If you have not yet done so, please run through this tutorial -- which generates the training data for the spam classification task we will use in this tutorial -- and execute the very last cell.  Once complete, you should see three `.csv` files appear in the `./data/spam` subdirectory in this folder. 
#
# As a quick reminder, the spam detection task is as follows:

# %% [markdown]
# ### Task: Spam Detection

# %% [markdown]
# We use a [YouTube comments dataset](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) that consists of YouTube comments from 5 videos. The task is to classify each comment as being
#
# * **`HAM`**: comments relevant to the video (even very simple ones), or
# * **`SPAM`**: irrelevant (often trying to advertise something) or inappropriate messages
#
# For example, the following comments are `SPAM`:
#
#         "Subscribe to me for free Android games, apps.."
#
#         "Please check out my vidios"
#
#         "Subscribe to me and I'll subscribe back!!!"
#
# and these are `HAM`:
#
#         "3:46 so cute!"
#
#         "This looks so fun and it's a good song"
#
#         "This is a weird video."

# %% [markdown]
# ## 1. Loading Training Data
#
# Our first task is to load up the training, validation, and testing data we saved in Ludwig's native `.csv` format at the end of the spam tutorial:

# %%
import pandas as pd

DISPLAY_ALL_TEXT = False

pd.set_option("display.max_colwidth", 0 if DISPLAY_ALL_TEXT else 50)

df_train_filtered = pd.read_csv('data/spam/spam_train.csv')
df_dev = pd.read_csv('data/spam/spam_dev.csv')
df_test = pd.read_csv('data/spam/spam_test.csv')

# %%
df_train_filtered.head(5)

# %% [markdown]
# Notice that the dataframe containing our training set contains a column called `probs`, which we added in the very last cell of the spam tutorial.  This field contains the probabilistic label vector generated by the Snorkel label model as a space-separated list of float values, which aligns with the convention for the Ludwig [`vector features`](https://uber.github.io/ludwig/user_guide/#vector-features) we'll need to use to handle probabilistic labels.  Note that in our development and test sets, we have recorded the true label in this column using a one-hot encoding, with a value of 1 at the index of the true label and 0 otherwise.

# %% [markdown]
# ## 2. Training a Code-Free Deep Learning Classifier with Ludwig

# %% [markdown]
# We'll now use the noisy training labels we generated with Snorkel to train a neural network classifier for our task.  Notice that when we use this sort of model, we do not need to explicitly featurize the data!
#
# First, we'll suppress some verbose outputs and check to see if our Tensorflow installation recognizes an installed, CUDA-enabled GPU.  This is not necessary, but can decrease training time in practical settings by orders of magnitude

# %%
# This cell suppresses some verbose warnings -- you can ignore it
import warnings

warnings.filterwarnings("ignore")

# %%
# This cell checks to see if you have tensorflow-gpu & CUDA installed,
# and also have a GPU available; if so, it will print `GPU Available? True`

import os

os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import tensorflow as tf

gpu = tf.test.is_gpu_available()
print(f"GPU Available? {gpu}")

# %% [markdown]
# To define the type of model we wish to train in Ludwig, we provide a [`model defintion`](https://uber.github.io/ludwig/user_guide/#command-line-interface) file that will contain all required information for building and training the model.  We use this interface to create our model definition [file](ludwig/model/spam_model_definition.yaml), which is a relatively straightforward `.yaml` file defining a single-layer, bidirectional Long Short-Term Memory (LSTM) network, a common tool for text analysis.  We've loaded this `.yaml` file up below as a dictionary so you can see what it contains -- if you'd like to see what the actual file looks like, open up `model/spam_model_definition.yaml` in a text editor.  
#
# The main points to take away from this are:
#
# * Input Features: defines the type of input (`text`), the type of neural network to use (`lstm`), and the network architecture parametres (e.g. `embedding_size`)
# * Output Features: defines the type of problem that will be solved, the type of loss function to use, and the parameters of the last neural network layer
# * Training: defines training hyperparameters and scheduling
#
# Note that the `name` field of each feature corresponds to a column in the `.csv` files that contains the relevant data.
#
# More examples of and information about how to build model definition files can be found in the [Ludwig User Guide](https://uber.github.io/ludwig/examples/).

# %%
import yaml
with open('model/spam_model_definition.yaml') as fl:
    a = yaml.load(fl)

# %% [markdown]
# At this point, we have our `.csv` files for training and our `.yaml` file for model definition.  This is all we need to train a deep learning model in Ludwig, we can now just work just from the command line! 
#
# In `jupyter`, system commands can be provided using the `!`, so we'll use this for convenience.  Once we've written the model definition file, all we have to do to train a model (using a GPU if available), evaluate that model, and log all results in a consistent format using Ludwig is to run the command below. Note that all that is reuqired are the paths to the training, development, and test sets; the path to the model definition file; and the name of the experiment.

# %%
# !ludwig experiment \
#   --data_train_csv data/spam/spam_train.csv \
#   --data_validation_csv data/spam/spam_dev.csv \
#   --data_test_csv data/spam/spam_test.csv \
#   --model_definition_file model/spam_model_definition.yaml \
#   --output_directory results/spam_experiment_run \
#   --experiment_name trial_1

# %% [markdown]
# ## 3. Visualize Model Results

# %% [markdown]
# We can also use tools in Ludwig to visualize the training curves from the model run logs -- just load up the `.json` file with the training statistics, and run a quick visualization function as below.  Note that the validation and training loss track each other, indicating that this model likely has not overfit.  The accessibility of such common diagnostics is an advantage of focusing on model supervision, and letting modern deep learning software handle the model training procedures.

# %%
import json

# Opening log file from experiment
with open(f"results/spam_experiment_run/trial_1_run/training_statistics.json") as f:
    train_stats = json.load(f)

# USing ludwig to visualize the training curves
from ludwig.visualize import learning_curves

a = learning_curves(
    train_stats,
    "SPAM",
    model_names="trial_1_model",
    output_directory=None,
    file_format="pdf",
)

# %% [markdown]
# Finally, as before, we confirm that we've learned a useful spam classifier -- but this time, we didn't even need to featurize the data!

# %%
import numpy as np

# Loading probabilistic labels
ludwig_probs_test = np.load("results/spam_experiment_run/trial_1_run/probs_predictions.npy")
# Evaluating model with a cutoff of 0.5
ludwig_preds_test = ludwig_probs_test[:, 1] > 0.5
# Getting ground truth labels
Y_test = df_test.label.values

# Computing and printing test set accuracy
accuracy_test = np.mean(ludwig_preds_test == Y_test)
print(f"Ludwig Test Accuracy: {accuracy_test* 100:.1f}%")

# %% [markdown]
# ## Summary

# %% [markdown]
# In this tutorial, we accomplished the following:
#
# * We showed how programmatic Snorkel labels can be easily integrated into modern, code-free deep learning frameworks like [Ludwig](https://uber.github.io/ludwig)
# * We demonstrated how Ludwig's inbuilt tools can be used to visualize training performance
#
# Note that while we have mostly used Snorkel's labeling utility in this tutorial, the workflow using the data augmentation package would look no different; as long as the data is saved in the correct `.csv` format, Ludwig can interact with it seamlessly.

# %% [markdown]
# ### Next Steps

# %% [markdown]
# If you enjoyed this tutorial and want to learn more, check out the following:
#
# * Using Snorkel + text to supervise an image model via [cross-modal weak supervision](https://snorkel.org/use-cases/visual-relation-tutorial)
# * Using [data augmentation](https://snorkel.org/use-cases/02-spam-data-augmentation-tutorial) to improve training data quality
# * The [Ludwig User Guide](https://uber.github.io/ludwig/user_guide), which contains more detail on Ludwig capabilities and API usage
